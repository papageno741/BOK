{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO3K9mXEX4lr"
      },
      "outputs": [],
      "source": [
        "#mecab설치\n",
        "!pip install konlpy\n",
        "!sudo apt-get install curl git\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6pPXq9qYP7s"
      },
      "outputs": [],
      "source": [
        "#ekonlpy 설치\n",
        "!git clone https://github.com/entelecheia/eKoNLPy.git\n",
        "!cd eKoNLPy\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/miniproject/eKoNLPy')\n",
        "!pip install .\n",
        "!pip install . --upgrade (for upgrade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj0LYcPKZ34l"
      },
      "source": [
        "# news 기사 n-gram화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5t86mC7Z3yk"
      },
      "outputs": [],
      "source": [
        "# news 기사 n-gram화\n",
        "\n",
        "mpck = MPCK()\n",
        "\n",
        "def make_ngram(df):\n",
        "    try:\n",
        "        # content(내용) 컬럼의 text N-gram화\n",
        "        tokens=mpck.tokenize(df['content'])\n",
        "        ngrams=mpck.ngramize(tokens)\n",
        "        tokens_text=\",\".join(tokens)\n",
        "        ngrams_text=\",\".join(ngrams)\n",
        "        total_text=tokens_text+\",\"+ngrams_text\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        total_text=\"\"\n",
        "    return total_text\n",
        "\n",
        "df=pd.read_csv(r\"뉴스기사content.csv\")\n",
        "mpck = MPCK()\n",
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.reset_index(inplace=True,drop=True)\n",
        "df['ngrams']=df.apply(make_ngram,axis=1)\n",
        "df.to_csv(r\"뉴스기사n-gram.csv\", encoding='utf-8-sig', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC3GjEUFZ9Bf"
      },
      "source": [
        "# 데이터 병합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f8e-AuKYP5R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW-anhy5YP0Z"
      },
      "outputs": [],
      "source": [
        "# .csv 파일들 데이터프레임 concat\n",
        "\n",
        "import os\n",
        "해당 경로의 디렉토리 이동\n",
        "os.chdir('/content/drive/MyDrive/miniproject/mergy/mergy2')\n",
        "\n",
        "import pandas as pd\n",
        "list_files = os.listdir()\n",
        "#채권분석 리포트, 의사록, 네이버 뉴스 파일 공통 컬럼 (['date','content']) concat\n",
        "concat_df = pd.concat(map(pd.read_csv, list_files), ignore_index=True)\n",
        "\n",
        "# 전체 파일에서 날짜 형식이 다른거 통일\n",
        "for i in range(len(concat_df)):\n",
        "    s = concat_df['date'][i]\n",
        "    concat_df['date'][i] = s.replace('-','.')\n",
        "concat_df.sort_values(by=['date'], axis=0, inplace=True)\n",
        "concat_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLrPcIOCYPyI"
      },
      "outputs": [],
      "source": [
        "#concat된 df에 콜금리의 labeling 컬럼 추가 (date 기준으로 merge)\n",
        "df_call = pd.read_csv('/content/drive/MyDrive/miniproject/callfinal1.csv', encoding='utf-8')\n",
        "merge_df = pd.merge(concat_df, df_call, on=\"date\", how='right')\n",
        "merge_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs_6AZO4Yp7m"
      },
      "outputs": [],
      "source": [
        "#필요없는 컬럼 정리, dropna, 중복값 정리\n",
        "merge_df = merge_df.drop(['Unnamed: 0',  'Unnamed: 0.1'], axis=1)\n",
        "merge_df = merge_df.drop(['callrate_x',  '한달전 date_x', '한달전 callrate_x', 'labeling_x', 'callrate_y', '한달전 date_y'], axis=1)\n",
        "merge_df = merge_df.drop(['한달전 callrate_y'], axis=1)\n",
        "merge_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIb-h1yoYp5h"
      },
      "outputs": [],
      "source": [
        "#라벨링 이름 원래대로 변경\n",
        "merge_df.rename(columns={'labeling_y':'labeling'}, inplace=True)\n",
        "merge_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgGzFkFhYp3V"
      },
      "outputs": [],
      "source": [
        "# 중복값 결측치 제거\n",
        "merge_df = merge_df.dropna(axis=0)\n",
        "merge_df = merge_df.drop_duplicates()\n",
        "merge_df.to_csv('/content/drive/MyDrive/miniproject/merge_final_ver3.csv',encoding='utf-8-sig',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkojeg-8ZwtY"
      },
      "source": [
        "\n",
        "# 의사록 한글자 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKnyD_w4Zwj2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "minutes = pd.read_csv('/content/drive/MyDrive/workspace/bok/minutes_data_save(08_21).csv',encoding='utf-8-sig')\n",
        "minutes.sort_values(by='date',ascending=True,inplace=True)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def remove_oneletter(text):\n",
        "    ngram_list = []\n",
        "    for j in range(len(text.split(','))):\n",
        "        ngram = text.split(',')[j]\n",
        "        ngram_list.append(ngram)\n",
        "        letter = ngram.split('/')[0]\n",
        "        letter2 = letter.split(';')[0]\n",
        "        if len(letter) == 1 or len(letter2) == 1:\n",
        "            # print(ngram)\n",
        "            for i,v in enumerate(ngram_list) :\n",
        "                if v == ngram:\n",
        "                    ngram_list[i] = np.nan\n",
        "    # print(ngram_list)\n",
        "    removed = ','.join(ng for ng in ngram_list if ng is not np.nan)\n",
        "        \n",
        "    return removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ZVcbQwYp0_"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "removed_text = []\n",
        "for i in tqdm(range(len(minutes))):\n",
        "    before_text = minutes['ngrams'][i]\n",
        "    print(remove_oneletter(before_text))\n",
        "    removed_text.append(remove_oneletter(before_text))\n",
        "\n",
        "minutes['ngrams'] = removed_text\n",
        "minutes.to_csv('ngrams_final_except1.csv',encoding='utf-8-sig',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2. Pre-processing - contents concat_n-gram.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
