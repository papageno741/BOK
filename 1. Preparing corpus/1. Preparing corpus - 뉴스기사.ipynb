{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RGchs8Jjqu9K"},"outputs":[],"source":["# 1. Preparing corpus - 뉴스기사 크롤링 (네이버링크가 있는 신문사 - 연합뉴스, 이데일리)\n","\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import datetime\n","def naver_news_crawling(query, start_date, end_date, s_from, e_to, page):\n","    title_list=[]\n","    date_list=[]\n","    contents_list=[]\n","    #최대5페이지까지\n","    while page != 0:\n","        url=\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\"+query+\"&sort=0&photo=0&field=0&pd=3&ds=\"+start_date+\"&de=\"+ end_date +\"&cluster_rank=10&mynews=1&office_type=2&office_section_code=8&news_office_checked=1001&nso=so:r,p:from\"+s_from+\"to\"+e_to+\",a:all&start=\"+str(page)+\"&refresh_start=0\"\n","        header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n","        req = requests.get(url,headers=header)\n","        cont = req.text\n","        soup = BeautifulSoup(cont, 'html.parser')\n","        for urls in soup.select(\"a.info\"):\n","            try :\n","                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n","                    title_text, date_text, contents_text= get_news(urls[\"href\"])  \n","                    title_list.append(title_text)\n","                    date_list.append(date_text)\n","                    contents_list.append(contents_text)\n","            except Exception as e:\n","                print(e) \n","                continue"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1648993727106,"user":{"displayName":"배근우","userId":"02149756809168262451"},"user_tz":-540},"id":"u089L3jdrw3c","outputId":"857d51e6-f46b-4c97-b5c5-0e72a0f76437"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1001'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["news_company = {'연합뉴스':'1001','이데일리':'2227'}\n","news_company['연합뉴스']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0wcGmkcJ3Vo"},"outputs":[],"source":["# 1. Preparing corpus - 뉴스기사 크롤링 (네이버링크가 있는 신문사 - 연합뉴스, 이데일리)\n","\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import datetime\n","\n","def get_news(url):      \n","    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110                                   Safari/537.36'}\n","    html = requests.get(url, headers=header)\n","    bsoup = BeautifulSoup(html.content, 'html.parser')    \n","    # 기사 제목\n","    title = bsoup.select('h3#articleTitle')[0].text\n","    # 날짜 \n","    pdate = bsoup.select('.t11')[0].get_text()[:10]\n","    # 기사 본문\n","    content = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n","    ctext = content.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\") \n","    return title, pdate, ctext\n","\n","def naver_news_crawling(query, start_date, end_date, s_from, e_to, page):\n","    \n","    title_list=[]\n","    date_list=[]\n","    contents_list=[]\n","    \n","    # 언론사에 따라 url주소의 언론사 숫자 변경\n","    news_company = {'연합뉴스':'1001','이데일리':'2227'}\n","    while page != 0:\n","  \n","        url=\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\"+query+\"&sort=0&photo=0&field=0&pd=3&ds=\"+start_date\n","        +\"&de=\"+ end_date +\"&cluster_rank=10&mynews=1&office_type=2&office_section_code=8&\"\n","        +\"news_office_checked=\"+news_company['연합뉴스']+\"&nso=so:r,p:from\"+s_from+\"to\"+e_to+\",a:all&start=\"+str(page)+\"&refresh_start=0\"\n","        header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110                                   Safari/537.36'}\n","        req = requests.get(url,headers=header)\n","        cont = req.text\n","        soup = BeautifulSoup(cont, 'html.parser')\n","\n","        for urls in soup.select(\"a.info\"):\n","            try :\n","\n","\n","                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n","                   \n","                    title_text, date_text, contents_text= get_news(urls[\"href\"])\n","                    \n","                    title_list.append(title_text)\n","                    date_list.append(date_text)\n","                    contents_list.append(contents_text)\n","                    \n","            except Exception as e:\n","                print(e) \n","                continue\n","\n","        # 네이버 검색결과의 다음 페이지 버튼을 가져온다.\n","        btn_next = soup.select_one('#main_pack > div.api_sc_page_wrap > div > a.btn_next')\n","        # 다음 페이지 버튼이 없으면(검색 결과 없을 경우) 크롤링 하지 않고 다음 날짜 링크로 가기\n","        if btn_next == None:\n","            break\n","        \n","        # 페이지의 다음 버튼이 비활성화 될때(마지막 페이지)까지 페이지 넘기기\n","        is_last = btn_next['aria-disabled']\n","        # 마지막 페이지일 경우 다음 날짜로 가기 위해 while 종료\n","        if is_last == \"true\":\n","            break\n","        # 네이버 페이지가 1, 11, 21, 31 순으로 가기 때문에 10씩 더해준다.\n","        else:\n","            page += 10\n","\n","    # 데이터 프레임 형식으로 만들어 데이터 반환   \n","    result={}   \n","    result= {'date' : date_list, 'title':title_list,'contents': contents_list}\n","    \n","    df=[]\n","    df=pd.DataFrame(result)\n","    \n","    return df\n","\n","error_date=[]\n","\n","page = 1\n","query = \"금리\" \n","date = datetime.date(2021,1,1) #시작날짜\n","cur_date=date.strftime('%Y.%m.%d')\n","e_to = s_from = cur_date.replace(\".\",\"\")\n","while(cur_date!=\"2022.01.01\"):#끝날짜 \n","    try:\n","        print(\"\"+cur_date+\"날짜의 크롤링을 시작합니다\")\n","        new_df = naver_news_crawling(query, cur_date, cur_date, s_from, e_to, page)\n","        new_df=new_df.drop_duplicates()\n","\n","        if len(new_df) > 0:\n","            new_df['title'] = new_df['title'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n","            new_df['contents'] = new_df['contents'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n","            new_df.to_csv(r\"금리2021.csv\",encoding='utf-8-sig',index=False, header=False, mode='a')\n","         \n","        date += datetime.timedelta(days=1)\n","        cur_date=date.strftime('%Y.%m.%d')\n","        e_to = s_from = cur_date.replace(\".\",\"\")\n","        print(\"성공\")\n","    except Exception as e:\n","        error_date.append(cur_date)\n","        print(e)\n","        print(\"실패\")\n","        continue\n","    \n","print(\"크롤링을 종료합니다\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlaZYsaLoFX9"},"outputs":[],"source":["# 1. Preparing corpus - 뉴스기사 크롤링 (네이버링크가 없는 신문사 - 연합인포맥스)\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from tqdm import tqdm\n","\n","titles = []\n","dates = []\n","contents = []\n","first_page = 1\n","last_page = 1001\n","\n","for i in tqdm(range(first_page,last_page)):\n","    urls = []\n","    url = 'https://news.einfomax.co.kr/news/articleList.html?page='+str(i)+'&total=157314&box_idxno=&sc_area=A&view_type=sm&sc_section_code=&sc_level=&sc_article_type=&sc_sdate=2008-01-01&sc_edate=2021-12-31&sc_order_by=E&sc_word=%EA%B8%88%EB%A6%AC&sc_andor=OR&sc_word2='\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    news = soup.select('section[class=\"article-list-content type-sm text-left\"] div[class=\"list-block\"]')\n","\n","    for i in range(len(news)):\n","        data = soup.select_one('#user-container > div.float-center.max-width-1080 > div.user-content.list-wrap > section > article > div.article-list > section > div:nth-of-type('+str(+i+1)+') > p > a')\n","        urls.append('https://news.einfomax.co.kr'+(data['href']))\n","\n","    for href in urls:\n","        response = requests.get(href)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        title = soup.select_one('div[class=\"article-head-title\"]')\n","        date= soup.select('div[class=\"info-text\"] li')\n","        s = date[1].text\n","        content =  soup.select_one('#article-view-content-div')\n","        # 기사 날짜 정보 리스트에 append\n","        dates.append(s[4:15])\n","        # 기사 제목 정보 리스트에 append\n","        titles.append(title.text)\n","        # 기사 내용 정보 리스트에 append\n","        contents.append(content.text)\n","\n","news_df = pd.DataFrame({'title':titles, 'date':dates,'content':contents})\n","news_df.to_csv(f'인포맥스{first_page}-{last_page}.csv',encoding='utf-8-sig',index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOzQH7YYS9vYIAx+FiDbIgY","collapsed_sections":[],"mount_file_id":"1TSS5zpZPmCbolfCUDqICY2RWRM9kFGzg","name":"1. Preparing corpus - 뉴스기사.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
